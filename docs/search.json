[
  {
    "objectID": "two_way_anova_fixed_random.html",
    "href": "two_way_anova_fixed_random.html",
    "title": "Two-way ANOVA (fixed and random effects)",
    "section": "",
    "text": "Data we’ll use: andrew.csv (chap 9), boîte 9.1 p. 209 Quinn & Keough\n\nBox 9.1 Worked example of nested ANOVA: grazing by sea urchins\nAndrew & Underwood (1993) manipulated the density of sea urchins in the shallow subtidal region of a site near Sydney to test the effects on the percentage cover of filamentous algae (ALGAE). There were four urchin treatments (TREAT) (no urchins, 33% of original density, 66% of original density and 100% of orginal density). The treatments were replicated in four distinct patches (PATCH) (3–4 m2) per treatment and percentage cover of filamentous algae (response variable) was measured in five random quadrats (QUAD) per patch.\n\n\n\n\n\n\nNote\n\n\n\nThis is a nested design with treatment (fixed factor), patch nested within treatment (random factor) and quadrats as the residual.\n\n\n\n# get the data\ndf &lt;- read.csv(\"data/andrew.csv\")\n\n\n# view the data\nDT::datatable(df)\n\n\n\n\n\n\n\n\nPlot the data\n\nlibrary(ggplot2)\nggplot(df, aes(x = factor(PATCH), y = ALGAE, color = TREAT)) +\n  geom_point() +\n  facet_wrap(~ TREAT, labeller = label_both) +\n  labs(x = \"PATCH\", color = \"TREATMENT\")\n\n\n\n\nNull hypotheses\n\nNo difference in the mean amount of filamentous algae between the four sea urchin density treatments.\nNo difference in the mean amount of filamentous algae between all possible patches in any of the treatments.\n\n\n\nAnova with fixed and random effects (anovaMM)\n\n# make sure the factors are correct\ndf$PATCH &lt;- as.factor(df$PATCH)\ndf$TREAT &lt;- as.factor(df$TREAT)\n\n\nlibrary(VCA)\nmodel_vca &lt;- anovaMM(ALGAE ~ TREAT + (TREAT/PATCH), Data = df)\nmodel_vca\n\n\n\nAnalysis of Variance Table:\n---------------------------\n\n            DF      SS      MS  F value      Pr(&gt;F)    \nTREAT        3 14429.1 4809.71 16.10754 6.57940e-08 ***\nTREAT:PATCH 12 21242.0 1770.16  5.92821 8.32261e-07 ***\nerror       64 19110.4  298.60                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMean: 20.2625 (N = 80) \n\nExperimental Design: balanced  |  Method: ANOVA\n\n# plot the model\nplot(model_vca)\n\n\n\n# coefficients\ncoef(model_vca)\n\n               int           TREATcon           TREATrem         TREATt0.33 \n      2.116316e+01      -1.589053e+01       3.723684e+01      -1.730526e+00 \n        TREATt0.66    TREATcon:PATCH1    TREATcon:PATCH2    TREATcon:PATCH3 \n     -2.832156e-15      -3.672632e+00      -5.272632e+00      -4.272632e+00 \n   TREATcon:PATCH4  TREATt0.66:PATCH5  TREATt0.66:PATCH6  TREATt0.66:PATCH7 \n     -2.672632e+00       7.236842e+00       1.563684e+01      -2.016316e+01 \n TREATt0.66:PATCH8  TREATt0.33:PATCH9 TREATt0.33:PATCH10 TREATt0.33:PATCH11 \n     -1.163158e+00      -1.683263e+01      -1.943263e+01       1.816737e+01 \nTREATt0.33:PATCH12   TREATrem:PATCH13   TREATrem:PATCH14   TREATrem:PATCH15 \n      1.636737e+01      -2.420000e+01       3.600000e+00      -5.620000e+01 \n  TREATrem:PATCH16 \n      0.000000e+00 \n\n\n\n\nConfidence intervals\nFunction VCAinference constructs one- and two-sided confidence intervals, and performs Chi-Squared tests for total and error variance against claimed values for ‘VCA’ objects.\n\nVCAinference(model_vca)\n\n\n\n\nInference from Linear Model Fit:\n--------------------------------\n\n&gt; ANOVA Table:\n--------------\n\n            DF    SS   MS F value    Pr(&gt;F)    \nTREAT        3 14429 4810  16.108 6.579e-08 ***\nTREAT:PATCH 12 21242 1770   5.928 8.323e-07 ***\nerror       64 19110  299                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMean: 20.2625 (N = 80) \n\nExperimental Design: balanced  |  Method: ANOVA\n\n\n&gt; VC:\n-----\n      Estimate   CI LCL   CI UCL One-Sided LCL One-Sided UCL\ntotal    298.6 217.1536 436.5502      228.3877      410.1393\nerror    298.6 217.1536 436.5502      228.3877      410.1393\n\n&gt; SD:\n-----\n      Estimate  CI LCL  CI UCL One-Sided LCL One-Sided UCL\ntotal    17.28 14.7361 20.8938       15.1125       20.2519\nerror    17.28 14.7361 20.8938       15.1125       20.2519\n\n&gt; CV[%]:\n--------\n      Estimate  CI LCL   CI UCL One-Sided LCL One-Sided UCL\ntotal  85.2809 72.7261 103.1155       74.5836       99.9477\nerror  85.2809 72.7261 103.1155       74.5836       99.9477\n\n\n95% Confidence Level  \nSAS PROC MIXED method used for computing CIs \n\n\n\n\nUsing function lmer()\n\nlibrary(lme4)\nmodel &lt;- lme4::lmer(ALGAE ~ TREAT + (1 | TREAT/PATCH), data = df, REML=FALSE)\nmodel\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: ALGAE ~ TREAT + (1 | TREAT/PATCH)\n   Data: df\n      AIC       BIC    logLik  deviance  df.resid \n 720.8312  737.5054 -353.4156  706.8312        73 \nRandom effects:\n Groups      Name        Std.Dev.\n PATCH:TREAT (Intercept) 14.35   \n TREAT       (Intercept)  0.00   \n Residual                17.28   \nNumber of obs: 80, groups:  PATCH:TREAT, 16; TREAT, 4\nFixed Effects:\n(Intercept)     TREATrem   TREATt0.33   TREATt0.66  \n       1.30        37.90        17.70        20.25  \noptimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings \n\n\n\nThe fixed effects indicate how each treatment compares to the baseline level (intercept) of ALGAE. Each treatment increases the expected ALGAE level by the respective amounts.\nThe random effects suggest variability in ALGAE levels based on the combination of PATCH and TREAT, but no variability in the treatment effect itself (as indicated by the zero standard deviation for TREAT).\nThe residual standard deviation indicates how much variability remains in ALGAE after accounting for the fixed and random effects.\n\n\n\nConclusion\nOverall, this model suggests that while different treatments lead to significant changes in the response variable, the variability of these effects across patches is substantial, with no inherent random variability in treatment effects themselves."
  },
  {
    "objectID": "two-way-anova-random-block.html",
    "href": "two-way-anova-random-block.html",
    "title": "ANOVA with random blocks",
    "section": "",
    "text": "ANOVA with random blocks is a statistical technique used to analyze data where the experimental units are grouped into blocks, and these blocks are considered random effects. The main goal is to account for variability due to the blocks (which could be differences between groups or environments) and test the significance of the fixed effects (e.g., treatments) while recognizing that blocks contribute random variation.\nThis type of design is commonly referred to as a Randomized Block Design (RBD) with random blocks.\n\nKey Concepts\n\nFixed Effects: These are the factors or treatments you are primarily interested in studying. For example, if you’re testing the effect of fertilizers on crop yield, the different types of fertilizers would be your fixed effect.\nRandom Effects (Blocks): These represent groups or experimental units that you want to control for. In a randomized block design, each block can represent something like a different field, batch, location, or time period, which could introduce variability. Blocks are assumed to be random samples from a larger population.\nWhy Use Random Blocks: Random blocks help account for variability in the response variable that is due to differences between blocks, which you are not directly interested in. By including random blocks in the model, you reduce the noise from these sources of variation and increase the precision of your estimates for the fixed effects (treatments).\n\n\n\n\n\n\n\nNote\n\n\n\nData we’ll use: walter.csv, boîte 10.1 p.264\n\n\n\n\nBox 10.1 Worked example of randomized complete block analysis: mites on leaves\nWalter & O’Dowd (1992) examined the role of domatia (small shelters at the juncture of veins on leaves) in determining the numbers of mites on leaves of plant species with domatia. They did an experiment using 14 pairs of leaves (randomly chosen) with one leaf in each pair with shaved domatia and the other as a control (normal domatia).\n\n# get thedata\ndf &lt;- read.csv(\"data/walter.csv\")\n\n\n# view the data\nDT::datatable(df)\n\n\n\n\n\n\n\n\nThe response variable was total number of mites per leaf, which Walter & O’Dowd (1992) transformed to \\(log_e(0.5+(mite \\times 10))\\), ostensibly to improve normality and homogeneity of variances between treatments, the 0.5 added because of zeros although multiplication by ten seemed unnecessary. The data were analyzed using model 10.1, the factors being block (BLOCK) and treatment (TREAT) and the response variable (LMITE) being \\(log_e(0.5+(mite \\times 10))\\).\nMITE is already transformed as LMITE. However, if we wanted to do it in R, we would have done:\n\n# base \"e\"\nlog(0.5+(df$MITE*10))\n\n [1]  4.5053499  2.3513753  3.0204249  2.3513753 -0.6931472  3.0204249\n [7]  4.7916498  3.7013020  5.0139631  3.0204249  3.4177267  2.3513753\n[13]  4.7050155 -0.6931472  4.1026434 -0.6931472  4.2556127  2.3513753\n[19]  4.1026434 -0.6931472  3.9219733  2.3513753  4.3882572  2.3513753\n[25]  3.4177267  2.3513753  4.1026434 -0.6931472\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we used log10(), the result would be wrong, because it has base 10.\n\n\n\n\nMake sure the variables are factors\n\ndf$BLOCK &lt;- as.factor(df$BLOCK)\ndf$TREAT &lt;- as.factor(df$TREAT)\n\n\n\nANOVA with random blocks\nThe table presented in the book:\n\n\n\nSource\nSS\ndf\nMS\nF\nP\n\n\n\n\nTreatment\n31.341\n1\n31.341\n11.315\n0.005\n\n\nBlock (leaf pair)\n23.058\n13\n1.774\n0.640\n0.784\n\n\nResidual\n36.007\n13\n2.770\n\n\n\n\n\nLet’s get it in R!\nWe’ll use the function anovaMM() from the package VCA. For this function, all the random terms need to be enclosed by round brackets ().\n\nlibrary(VCA)\nmodel &lt;- anovaMM(LMITE ~ TREAT + (BLOCK), Data = df)\nmodel\n\n\n\nANOVA-Type Estimation of Mixed Model:\n--------------------------------------\n\n    [Fixed Effects]\n\n     int   TREAT1   TREAT2 \n1.673514 2.115949 0.000000 \n\n\n    [Variance Components]\n\n  Name  DF SS        MS       VC       %Total SD       CV[%]    \n1 total 26                    2.769788 100    1.664268 60.928984\n2 BLOCK 13 23.057582 1.77366  0*       0*     0*       0*       \n3 error 13 36.007247 2.769788 2.769788 100    1.664268 60.928984\n\nMean: 2.731488 (N = 28) \n\nExperimental Design: balanced  |  Method: ANOVA | * VC set to 0 | adapted MS used for total DF\n\n\n\n\nInterpretation\nThis output presents the results of an ANOVA-type estimation of a mixed-effects model, with both fixed effects and random effects (variance components). Here’s a breakdown of the key components of the result:\n\n\n1. Fixed Effects\nThe first part of the output shows the estimates of the fixed effects, which include the intercept and the treatment levels (TREAT):\n\n\n\nParameter\nEstimate\n\n\n\n\nIntercept\n1.673514\n\n\nTREAT1\n2.115949\n\n\nTREAT2\n0.000000\n\n\n\n\nIntercept (1.673514): This is the estimated mean response when the treatment level is at the baseline (reference category).\nTREAT1 (2.115949): This is the estimated difference in response between the baseline treatment and TREAT1. It means that TREAT1 is associated with an increase of 2.115949 units compared to the baseline.\nTREAT2 (0.000000): This indicates that TREAT2 is the baseline treatment, hence the estimate is 0.\n\n\n\n2. Variance Components\nThe second part lists the variance components, which describe the variability in the model, including the total variance, block variance, and residual (error) variance.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent\nDF\nSS\nMS\nVC\n%Total\nSD\nCV[%]\n\n\n\n\ntotal\n26\n\n\n2.769788\n100\n1.664268\n60.928984\n\n\nBLOCK\n13\n23.057582\n1.77366\n0*\n0*\n0*\n0*\n\n\nerror\n13\n36.007247\n2.769788\n2.769788\n100\n1.664268\n60.928984\n\n\n\n\n\nDefinitions:\n\nDF (Degrees of Freedom): Indicates how much information is available to estimate the variance. Here, 13 degrees of freedom are associated with BLOCK and 13 with the residual (error).\nSS (Sum of Squares): Reflects the total variability explained by each component. The larger the sum of squares, the more variability the component accounts for.\nMS (Mean Squares): The mean of the squares (SS divided by DF). This is used to assess the contribution of each component.\nVC (Variance Component): Represents the estimated variance attributed to each component.\n\nFor BLOCK, the variance component is set to 0 (* VC set to 0). This means that the model found no significant variance attributable to the blocks, which can happen when there is no substantial variability between blocks.\nFor error, the variance component is 2.769788, representing the residual variance (unexplained variation).\n\n%Total: The percentage of total variance explained by each component. In this case, all the variance (100%) is attributed to the residual error, and none is attributed to BLOCK.\nSD (Standard Deviation): The square root of the variance component. This provides the estimated standard deviation for each variance component.\nCV[%] (Coefficient of Variation): The ratio of the standard deviation to the mean, expressed as a percentage. This measures the relative variability.\n\n\n\n3. Other Metrics\n\nMean: The overall mean response in the dataset is 2.731488, based on 28 observations (N = 28).\nExperimental Design: balanced: Indicates that the design is balanced, meaning each treatment is equally represented across the blocks.\nMethod: ANOVA: The estimation method used is ANOVA.\nVC set to 0: The variance component for BLOCK was set to 0, indicating that the random block effect was found to be negligible.\n\n\n\nInterpretation:\n\nFixed Effects: The treatment effect (TREAT1) is estimated to increase the response by 2.115949 units compared to the baseline (TREAT2), which is set to 0.\nVariance Components: The random effect of BLOCK has a variance component of 0, meaning that BLOCK does not explain any additional variability in the model. The residual variance (error) is the only contributor to the total variability in the model.\nDesign: The model assumes a balanced design, and the analysis method used is ANOVA.\n\nIn summary, this output suggests that the block structure does not contribute significantly to the variability in the response, and the treatment effects are the primary source of systematic variation. All the variability in the model is attributed to the residual error.\n\n\nAnother function\nWe can also use the function lmer() from the package lme4 and obtain more results.\n\nlibrary(lme4)\nlibrary(lmerTest)\nmodel &lt;- lmer(LMITE ~ TREAT + (1|BLOCK), data = df, REML=F)\nmodel\n\nLinear mixed model fit by maximum likelihood  ['lmerModLmerTest']\nFormula: LMITE ~ TREAT + (1 | BLOCK)\n   Data: df\n     AIC      BIC   logLik deviance df.resid \n108.3606 113.6894 -50.1803 100.3606       24 \nRandom effects:\n Groups   Name        Std.Dev.\n BLOCK    (Intercept) 0.000   \n Residual             1.452   \nNumber of obs: 28, groups:  BLOCK, 14\nFixed Effects:\n(Intercept)       TREAT2  \n      3.789       -2.116  \noptimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings \n\nanova(model)\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nTREAT 31.341  31.341     1    28  14.857 0.0006199 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: LMITE ~ TREAT + (1 | BLOCK)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n   108.4    113.7    -50.2    100.4       24 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0863 -0.2560  0.4395  0.4733  1.3962 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n BLOCK    (Intercept) 0.000    0.000   \n Residual             2.109    1.452   \nNumber of obs: 28, groups:  BLOCK, 14\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)   3.7895     0.3882 28.0000   9.762 1.63e-10 ***\nTREAT2       -2.1159     0.5490 28.0000  -3.855  0.00062 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nTREAT2 -0.707\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "oneway_anova.html",
    "href": "oneway_anova.html",
    "title": "One-way ANOVA",
    "section": "",
    "text": "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups to determine if there are significant differences among them. It helps test the hypothesis that the means of different groups are equal. ANOVA essentially examines the variation within groups and between groups to assess whether the observed differences in sample means are due to chance or reflect true differences in the population.\n\nAssumptions\nANOVA relies on several key assumptions to ensure the validity of its results. If these assumptions are violated, the results of the ANOVA test may not be reliable. Here are the main assumptions:\n\n1. Independence of Observations:\n\nEach observation in the dataset must be independent of all others. This means that the outcome of one observation should not influence the outcome of another.\nHow to check: This assumption is generally met by ensuring a proper study design (e.g., random sampling or random assignment).\n\n\n\n2. Normality:\n\nThe residuals (differences between observed and predicted values) within each group should be approximately normally distributed.\nHow to check: You can assess normality by using:\n\nA Q-Q plot (quantile-quantile plot) of the residuals.\nStatistical tests like the Shapiro-Wilk test or Kolmogorov-Smirnov test (though these are sensitive in large datasets).\n\n\nNote: ANOVA is robust to moderate deviations from normality, especially when sample sizes are large and approximately equal across groups.\n\n\n3. Homogeneity of Variances (Homoscedasticity):\n\nThe variance within each group should be approximately equal (i.e., the spread of the data within each group should be similar).\nHow to check:\n\nUse Levene’s Test or Bartlett’s Test to formally test for equality of variances.\nAlternatively, inspect boxplots or use graphical methods to evaluate variance homogeneity.\n\n\n\n\n4. Additivity and Linearity:\n\nThe relationship between the dependent variable and the predictors (in the case of two-way ANOVA) should be additive and linear.\nAdditivity implies that the effect of one factor should not depend on the levels of another factor (no interaction effects).\nLinearity assumes a linear relationship between the predictors and the outcome variable.\n\n\n\n5. Fixed Effects:\n\nANOVA assumes that the factors or groups being analyzed are fixed effects, meaning they are pre-determined and not randomly sampled from a larger population.\n\n\n\nConsequences of Violating Assumptions:\n\nViolation of Independence: This is critical. Violations can severely compromise the test’s results, leading to misleading conclusions.\nViolation of Normality: If violated and the sample size is small, the ANOVA may produce inaccurate p-values. For larger sample sizes, ANOVA is relatively robust to this violation.\nViolation of Homogeneity of Variances: This can inflate the type I error rate (false positives) or type II error rate (false negatives). Use Welch’s ANOVA if this assumption is violated.\n\n\n\nHow to Address Violations:\n\nTransformations (e.g., log or square root transformations) can sometimes address issues with normality or unequal variances.\nUse non-parametric alternatives like the Kruskal-Wallis test when normality and homoscedasticity assumptions are strongly violated.\nIf variances are unequal, use Welch’s ANOVA as a more robust alternative to traditional ANOVA.\n\nBy ensuring these assumptions are met (or using appropriate alternatives when they are violated), you can make valid inferences from ANOVA results.\nData we’ll use: medley.csv (chap 8), boîte 8.1 p.174 Quinn & Keough\n\n\n\nBox 8.1 Worked example: diatom communities in metalaffected streams\nMedley & Clements (1998) sampled a number of stations (between four and seven) on six streams known to be polluted by heavy metals in the Rocky Mountain region of Colorado, USA. They recorded zinc concentration, and species richness and species diversity of the diatom community and proportion of diatom cells that were the early-successional species, Achanthes minutissima.\n\nSpecies diversity versus zinc-level group\n\n# get thedata\ndf &lt;- read.csv(\"data/medley.csv\")\n\n\nDT::datatable(df)\n\n\n\n\n\n\n\n# STREAM has 6 groups\ntable(df$STREAM)\n\n\nArkan  Blue Chalk Eagle Snake Splat \n    7     7     5     4     5     6 \n\n# ZINC has 4 groups\ntable(df$ZINC)\n\n\nBACK HIGH  LOW  MED \n   8    9    8    9 \n\n\n\n\n\nCalculating ANOVA\nThe first analysis compares mean diatom species diversity (response variable) across the four zinc-level groups (categorical predictor variable), zinc level treated as a fixed factor. The \\(H_0\\) was no difference in mean diatom species diversity between zinc-level groups. The results from an analysis of variance from fitting a linear model with zinc level as the predictor variable were as follows.\n\n\n\nSource\nSS\ndf\nMS\nF\nP-value\n\n\n\n\nZinc level\n2.567\n3\n0.856\n3.939\n0.018\n\n\nResidual\n6.516\n30\n0.217\n\n\n\n\nTotal\n9.083\n33\n\n\n\n\n\n\n\\(H_0\\): The means of the different groups are the same.\n\\(H_1\\): At least one sample mean is not equal to the others.\nP-value &lt; 0.05 = Reject \\(H_0\\).\n\nmodel1 &lt;- lm(DIVERSTY ~ ZINC, data = df)\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: DIVERSTY\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nZINC       3 2.5666 0.85554  3.9387 0.01756 *\nResiduals 30 6.5164 0.21721                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nOther way of getting ANOVA\n\nmodel2 &lt;- aov(DIVERSTY ~ ZINC, data = df)\nsummary(model2)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nZINC         3  2.567  0.8555   3.939 0.0176 *\nResiduals   30  6.516  0.2172                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nChecking ZINC groups in terms of DIVERSTY\n\nlibrary(ggplot2)\nggplot(data = df, aes(x = ZINC, y = DIVERSTY)) +\n  geom_boxplot()\n\n\n\n\nBoxplots of species diversity against group showed no obvious skewness. Two sites with low species diversity were highlighted in the background and medium zinc groups as possible outliers.\n\n\nResidual plot\n\nplot(model1, which = 1)\n\n\n\n\nThe residual plot from this model did not reveal any outliers or any unequal spread of the residuals, suggesting the assumptions of the ANOVA were appropriate.\n\n\nLevene-median test\n\\(H_0\\): no difference in variances of species diversity between zinc-level groups\n\\(H_1\\): different variances of species diversity between zinc-level groups\n\nlibrary(car)\nleveneTest(DIVERSTY ~ ZINC, data = df, center = \"median\")\n\nLevene's Test for Homogeneity of Variance (center = \"median\")\n      Df F value Pr(&gt;F)\ngroup  3  0.0195 0.9962\n      30               \n\n\nP-value &gt; 0.05: \\(H_0\\) was not rejected.\n\n\nLevene-mean test\n\\(H_0\\): no difference in variances of species diversity between zinc-level groups \\(H_1\\): different variances of species diversity between zinc-level groups\n\nleveneTest(DIVERSTY ~ ZINC, data = df, center = \"mean\")\n\nLevene's Test for Homogeneity of Variance (center = \"mean\")\n      Df F value Pr(&gt;F)\ngroup  3  0.0867 0.9668\n      30               \n\n\nP-value &gt; 0.05: \\(H_0\\) was not rejected.\nAdditionally, Levene’s test produced no evidence that the \\(H_0\\) of no differences in variances of species diversity between the zinc-level groups should be rejected (Levene-mean: \\(F_{3,30} = 0.087, P = 0.967\\); Levene-median: \\(F_{3,30} = 0.020, P = 0.996\\))\n\n\nTukey’s pairwise comparison\nTukey’s pairwise comparison of group means: mean differences with Tukey adjusted P-values for each pairwise comparison in brackets.\n\n\n\n\n\n\n\n\n\n\n\n\nBackground\nLow\nMedium\nHigh\n\n\n\n\nBackground\n0.000 (1.000)\n\n\n\n\n\nLow\n0.235 (0.746)\n0.000 (1.000)\n\n\n\n\nMedium\n0.080 (0.985)\n0.315 (0.515)\n0.000 (1.000)\n\n\n\nHigh\n0.520 (0.122)\n0.755 (0.012)\n0.440 (0.209)\n0.000 (1.000)\n\n\n\nLet’s check in R!\n\n# we need to use the second model with the function aov()\nTukeyHSD(model2, conf.level=.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = DIVERSTY ~ ZINC, data = df)\n\n$ZINC\n                 diff        lwr        upr     p adj\nHIGH-BACK -0.51972222 -1.1355064 0.09606192 0.1218677\nLOW-BACK   0.23500000 -0.3986367 0.86863665 0.7457444\nMED-BACK  -0.07972222 -0.6955064 0.53606192 0.9847376\nLOW-HIGH   0.75472222  0.1389381 1.37050636 0.0116543\nMED-HIGH   0.44000000 -0.1573984 1.03739837 0.2095597\nMED-LOW   -0.31472222 -0.9305064 0.30106192 0.5153456\n\n\n\nplot(TukeyHSD(model2, conf.level=.95), las = 2)\n\n\n\n\nThe only \\(H_0\\) to be rejected is that of no difference in diatom diversity between sites with low zinc and sites with high zinc. We could also analyze these data with more robust methods, especially if we were concerned about underlying non-normality or outliers.\n\n\nKruskal-Wallis rank sum test (non-parametric)\nTo test the \\(H_0\\) that there is no difference in the location of the distributions of diatom diversity between zinc levels, irrespective of the shape of these distributions, we would use the Kruskal–Wallis non-parametric test based on ranks sums.\n\nkruskal.test(DIVERSTY ~ ZINC, data = df)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  DIVERSTY by ZINC\nKruskal-Wallis chi-squared = 8.7367, df = 3, p-value = 0.033\n\n# get the rank sum\ndf$rank &lt;- rank(df$DIVERSTY)\naggregate(rank ~ ZINC, data = df, sum)\n\n  ZINC  rank\n1 BACK 160.0\n2 HIGH  85.5\n3  LOW 183.0\n4  MED 166.5\n\n\n\n\n\nZinc level\nRank sum\n\n\n\n\nBackground\n160.0\n\n\nLow\n183.0\n\n\nMedium\n166.5\n\n\nHigh\n85.5\n\n\n\nThe Kruskal–Wallis H-statistic equals 8.737. The probability of getting this value of one more extreme when the \\(H_0\\) is true (testing with a chi-square distribution with 3 df ) is 0.033, so we would reject the \\(H_0\\).\n\n\nPairwise Wilcoxon Rank Sum Tests\nFrom the output of the Kruskal-Wallis test, we know that there is a significant difference between groups, but we don’t know which pairs of groups are different.\n\npairwise.wilcox.test(df$DIVERSTY, df$ZINC, p.adjust.method = \"none\")\n\n\n    Pairwise comparisons using Wilcoxon rank sum exact test \n\ndata:  df$DIVERSTY and df$ZINC \n\n     BACK   HIGH   LOW   \nHIGH 0.0274 -      -     \nLOW  0.6454 0.0093 -     \nMED  0.8884 0.0625 0.3356\n\nP value adjustment method: none \n\n\nGroups with p-value &lt; 0.05 are significantly different."
  },
  {
    "objectID": "mutiple_linear_regression.html",
    "href": "mutiple_linear_regression.html",
    "title": "Régression multiple lineaire (6.1, 6.2)",
    "section": "",
    "text": "L’équation de régression multiple peut être écrite comme suit :\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n + \\epsilon\n\\]\nOù :\n\n\\(Y\\) est la variable dépendante (le résultat que vous prédisez),\n\\(\\beta_0\\) est l’interception (la valeur de \\(Y\\) lorsque toutes les variables \\(X\\) sont égales à 0),\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_n\\) sont les coefficients des variables indépendantes,\n\\(X_1, X_2, \\ldots, X_n\\) sont les variables indépendantes,\n\\(\\epsilon\\) est le terme d’erreur (la différence entre les valeurs réelles et prédites).\n\nWe’ll be using the dataset paruelo.csv (chap 6), boîte 6.1 p. 112.\nBox 6.1 Worked example of multiple linear regression: relative abundance of plant functional types\n\nData\nParuelo & Lauenroth (1996) analyzed the geographic distribution and the effects of climate variables on the relative abundance of a number of plant functional types (PFTs) including shrubs, forbs, succulents (e.g. cacti), \\(C_3\\) grasses and \\(C_4\\) grasses. There were 73 sites across North America. The variables of interest are the relative abundance of \\(C_3\\) plants, the latitude in centesimal degrees (LAT), the longitude in centesimal degrees (LONG), the mean annual precipitation in mm (MAP), the mean annual temperature in \\(°C\\) (MAT), the proportion of MAP that fell in June, July and August (JJAMAP) and the proportion of MAP that fell in December, January and February (DJFMAP).\n\n# get thedata\ndf &lt;- read.csv(\"data/paruelo.csv\")\n\n\nDT::datatable(df)\n\n\n\n\n\n\n\n\nDistribution of C3\n\nhist(df$C3)\n\n\n\n\nThe relative abundance of \\(C_3\\) plants was positively skewed and transformed to \\(log_{10} + 0.1\\) (\\(log_{10}C_3\\)). A correlation matrix between the predictor variables indicated that some predictors are strongly correlated.\nThe transformation is seen in the column LC3.\nIf not, we could have transformed this way in R.\n\nlog10(df$C3 + 0.1)\n\n [1] -0.124938737 -0.124938737 -0.065501549 -0.070581074 -0.366531544\n [6] -0.886056648 -1.000000000 -0.920818754 -0.823908741 -0.823908741\n[11] -0.337242168 -1.000000000 -0.508638306 -0.214670165 -0.769551079\n[16] -0.408935393 -0.823908741 -0.638272164 -1.000000000 -0.124938737\n[21] -1.000000000 -0.004364805 -0.744727495 -0.244125144 -1.000000000\n[26] -1.000000000 -0.508638306 -0.408935393 -0.346787486 -0.420216403\n[31] -0.259637311 -0.387216143 -0.920818754 -0.229147988 -0.337242168\n[36] -0.958607315 -1.000000000 -1.000000000 -0.136677140 -0.886056648\n[41] -0.769551079 -0.657577319 -0.337242168 -0.136677140 -0.017728767\n[46] -0.244125144 -0.167491087 -0.107905397 -0.091514981 -0.795880017\n[51] -0.853871964 -0.619788758 -0.795880017 -0.356547324 -0.387216143\n[56] -0.508638306 -0.102372909 -0.236572006 -0.920818754 -0.744727495\n[61] -0.677780705 -0.677780705 -0.481486060 -0.552841969 -0.823908741\n[66] -0.537602002 -0.236572006 -0.920818754 -1.000000000 -1.000000000\n[71] -0.769551079 -0.387216143 -0.086186148\n\n\n\n\nDistribution of LC3 (Y variable)\n\nhist(df$LC3)\n\n\n\n\n\n\nCorrelation matrix (plot) between the predictors\n\nlibrary(\"PerformanceAnalytics\")\nchart.Correlation(df[,3:8], histogram=TRUE, pch=19)\n\n\n\n\n\n\nCorrelation matrix between the predictors\n\ncor(df[,c(\"LAT\",\"LONG\",\"MAP\",\"MAT\",\"JJAMAP\",\"DJFMAP\")])\n\n               LAT        LONG        MAP          MAT      JJAMAP       DJFMAP\nLAT     1.00000000  0.09655281 -0.2465058 -0.838590413  0.07417497 -0.065124848\nLONG    0.09655281  1.00000000 -0.7336870 -0.213109100 -0.49155774  0.770743994\nMAP    -0.24650582 -0.73368703  1.0000000  0.355090766  0.11225905 -0.404512409\nMAT    -0.83859041 -0.21310910  0.3550908  1.000000000 -0.08077131  0.001478037\nJJAMAP  0.07417497 -0.49155774  0.1122590 -0.080771307  1.00000000 -0.791540381\nDJFMAP -0.06512485  0.77074399 -0.4045124  0.001478037 -0.79154038  1.000000000\n\n\nA correlation matrix between the predictor variables indicated that some predictors are strongly correlated.\n\n\n\n\nLAT\nLONG\nMAP\nMAT\nJJAMAP\nDJFMAP\n\n\nLAT\n1.000\n\n\n\n\n\n\n\nLONG\n0.097\n1.000\n\n\n\n\n\n\nMAP\n-0.247\n-0.734\n1.000\n\n\n\n\n\nMAT\n-0.839\n-0.213\n0.355\n1.000\n\n\n\n\nJJAMAP\n0.074\n-0.492\n0.112\n-0.081\n1.000\n\n\n\nDJFMAP\n-0.065\n0.771\n-0.405\n0.001\n-0.792\n1.000\n\n\n\nNote the high correlations between LAT and MAT, LONG and MAP, and JJAMAP and DJFMAP, suggesting that collinearity may be a problem with this analysis.\n\n\nModel 1: additive model\nWith six predictor variables, a linear model with all possible interactions would have 64 model terms (plus an intercept) including four-, five- and six-way interactions that are extremely difficult to interpret. As a first pass, we fitted an additive model:\n\\[(log_{10}C_3) = \\beta_0 + \\beta_1(LAT)_i + \\beta_2(LONG)_i + \\beta_3(MAP)_i + \\beta_4(MAT)_i + \\beta_5(JJAMAP)_i + \\beta_6(DJFMAP)_i + \\epsilon_i\\]\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStandard error\nStandardized coefficient\nTolerance\nt\nP\n\n\n\n\nIntercept\n-2.689\n1.239\n0\n\n-2.170\n0.034\n\n\nLAT\n0.043\n0.010\n0.703\n0.285\n4.375\n&lt;0.001\n\n\nLONG\n0.007\n0.010\n0.136\n0.190\n0.690\n0.942\n\n\nMAP\n&lt;0.001\n&lt;0.001\n0.181\n0.357\n1.261\n0.212\n\n\nMAT\n-0.001\n0.012\n-0.012\n0.267\n-0.073\n0.942\n\n\nJJAMAP\n-0.834\n0.475\n-0.268\n0.316\n-1.755\n0.084\n\n\nDJFMAP\n-0.962\n0.716\n-0.275\n0.175\n-1.343\n0.184\n\n\n\nLet’s do the same thing in R!\n\n\nModel 1: all the explanatory variables\n\nmodel1 &lt;- lm(LC3 ~ LAT + LONG + MAP + MAT + JJAMAP + DJFMAP, data = df)\nsummary(model1)\n\n\nCall:\nlm(formula = LC3 ~ LAT + LONG + MAP + MAT + JJAMAP + DJFMAP, \n    data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55607 -0.17084  0.02938  0.18397  0.47953 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.6886133  1.2391496  -2.170   0.0336 *  \nLAT          0.0434331  0.0099285   4.375 4.42e-05 ***\nLONG         0.0069236  0.0100330   0.690   0.4926    \nMAP          0.0002743  0.0002175   1.261   0.2117    \nMAT         -0.0008468  0.0116261  -0.073   0.9422    \nJJAMAP      -0.8338536  0.4750796  -1.755   0.0839 .  \nDJFMAP      -0.9618361  0.7163073  -1.343   0.1839    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2387 on 66 degrees of freedom\nMultiple R-squared:  0.5136,    Adjusted R-squared:  0.4694 \nF-statistic: 11.61 on 6 and 66 DF,  p-value: 7.787e-09\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: LC3\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nLAT        1 3.5703  3.5703 62.6573 3.749e-11 ***\nLONG       1 0.0412  0.0412  0.7238   0.39796    \nMAP        1 0.1818  0.1818  3.1901   0.07868 .  \nMAT        1 0.0002  0.0002  0.0036   0.95224    \nJJAMAP     1 0.0745  0.0745  1.3066   0.25714    \nDJFMAP     1 0.1027  0.1027  1.8030   0.18395    \nResiduals 66 3.7608  0.0570                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFinal equation for the model 1 (All the variables):\n\n\n\\[\n\\operatorname{LC3} = \\beta_{0} + \\beta_{1}(\\operatorname{LAT}) + \\beta_{2}(\\operatorname{LONG}) + \\beta_{3}(\\operatorname{MAP}) + \\beta_{4}(\\operatorname{MAT}) + \\beta_{5}(\\operatorname{JJAMAP}) + \\beta_{6}(\\operatorname{DJFMAP}) + \\epsilon\n\\]\n\n\n\\[\n\\operatorname{\\widehat{LC3}} = -2.69 + 0.04(\\operatorname{LAT}) + 0.01(\\operatorname{LONG}) + 0(\\operatorname{MAP}) + 0(\\operatorname{MAT}) - 0.83(\\operatorname{JJAMAP}) - 0.96(\\operatorname{DJFMAP})\n\\]\n\n\n\n\nTolerance\nIn multiple regression analysis, Variance Inflation Factor (VIF) and Tolerance are both used to detect multicollinearity, a condition where independent variables are highly correlated. Here’s what they represent:\n\nVariance Inflation Factor (VIF):\n\nDefinition: VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity among the predictor variables. The higher the VIF, the more collinear the variable is with others.\nInterpretation:\n\nVIF = 1: No correlation between the predictor and the other variables.\nVIF between 1 and 5: Moderate correlation; not usually a cause for concern.\nVIF &gt; 5 or 10: High correlation; multicollinearity might be a problem that could distort the results.\n\n\n\n\nTolerance:\n\nDefinition: Tolerance is the reciprocal of VIF and measures the proportion of variance in a predictor that is not explained by the other predictors. It indicates how much of the variability of the independent variable is unique.\nTolerance = 1/VIF​\nInterpretation:\n\nTolerance near 1: Little or no multicollinearity.\nTolerance &lt; 0.2 or 0.1: Indicates potential multicollinearity issues.\n\n\n\n\nRelationship:\n\nWhen VIF is large, Tolerance is small.\n\nFor example, if VIF is 5, Tolerance will be 0.2. If VIF is 10, Tolerance will be 0.1. Both VIF and Tolerance help in diagnosing multicollinearity, but using one is usually enough as they are inversely related.\n\nlibrary(car)\n# Variance Inflation Factors (VIF)\nvif(model1)\n\n     LAT     LONG      MAP      MAT   JJAMAP   DJFMAP \n3.502732 5.267618 2.799428 3.742780 3.163215 5.710315 \n\n# tolerance\ntolerance &lt;- 1/vif(model1)\ntolerance\n\n      LAT      LONG       MAP       MAT    JJAMAP    DJFMAP \n0.2854914 0.1898391 0.3572159 0.2671810 0.3161340 0.1751217 \n\n\nIt is clear that collinearity is a problem with tolerances for two of the predictors (LONG & DJFMAP) approaching 0.1.\n\n\n\nModel 2: with interaction\nParuelo & Lauenroth (1996) separated the predictors into two groups their analyses. One group included LAT and LONG and the other included MAP, MAT, JJAMAP and DJFMAP. We will focus on the relationship between log-transformed relative abundance of \\(C_3\\) plants and latitude and longitude. We fitted a multiplicative model including an interaction term that measured how the relationship between \\(C_3\\) plants and latitude could vary with longitude and vice versa:\n\\((log_{10}C_3)_i = \\beta_0 + \\beta_1(LAT)_i + \\beta_2(LONG)_i + \\beta_3(LAT\\times LONG)_i + \\epsilon_i\\)\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStandard error\nTolerance\nt\nP-value\n\n\n\n\nIntercept\n7.391\n3625\n\n2.039\n0.045\n\n\nLAT\n-0.191\n0.091\n0.003\n-2.102\n0.039\n\n\nLONG\n-0.093\n0.035\n0.015\n-2.659\n0.010\n\n\nLAT \\(\\times\\) LONG\n0.002\n0.001\n0.002\n2.572\n0.012\n\n\n\nLet’s do that in R!\n\nmodel2 &lt;- lm(LC3 ~ LAT*LONG, data = df)\nsummary(model2)\n\n\nCall:\nlm(formula = LC3 ~ LAT * LONG, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54185 -0.13298 -0.02287  0.16807  0.43410 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  7.3909798  3.6253486   2.039  0.04531 * \nLAT         -0.1912401  0.0910018  -2.101  0.03925 * \nLONG        -0.0929020  0.0349331  -2.659  0.00972 **\nLAT:LONG     0.0022522  0.0008757   2.572  0.01227 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2334 on 69 degrees of freedom\nMultiple R-squared:  0.5137,    Adjusted R-squared:  0.4926 \nF-statistic:  24.3 on 3 and 69 DF,  p-value: 7.657e-11\n\n\nFinal equation for the model 2 (Interaction):\n\n\n\\[\n\\operatorname{LC3} = \\beta_{0} + \\beta_{1}(\\operatorname{LAT}) + \\beta_{2}(\\operatorname{LONG}) + \\beta_{3}(\\operatorname{LAT} \\times \\operatorname{LONG}) + \\epsilon\n\\]\n\n\n\\[\n\\operatorname{\\widehat{LC3}} = 7.39 - 0.19(\\operatorname{LAT}) - 0.09(\\operatorname{LONG}) + 0(\\operatorname{LAT} \\times \\operatorname{LONG})\n\\]\n\n\n\n# tolerance\n1/car::vif(model2)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n        LAT        LONG    LAT:LONG \n0.003249445 0.014973575 0.002494144 \n\n\nNote the very low tolerances indicating high correlations between the predictor variables and their interactions. An indication of the effect of collinearity is that if we omit the interaction and refit the model, the partial regression slope for latitude changes sign.\n\n\nModel 3: centering and interaction\nWe refitted the multiplicative model after centering both LAT and LONG.\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStandard error\nTolerance\nt\nP-value\n\n\n\n\nIntercept\n-0.553\n0.027\n\n20.130\n&lt;0.001\n\n\nLAT\n0.048\n0.006\n0.829\n8.483\n&lt;0.001\n\n\nLONG\n-0.003\n0.004\n0.980\n-0.597\n0.552\n\n\nLAT \\(\\times\\) LONG\n0.002\n0.001\n0.820\n2.572\n0.012\n\n\n\nLet’s do that in R!\n\n# center the variables LONG and LAT\n# note: the function c() was used only to structure the values from column to row \n# to occupy less space in the screen.\n\nc(scale(df$LONG, scale = F))\n\n [1]  13.14986301   7.86986301   4.37986301  -4.53013699  -3.58013699\n [6]  -7.02013699   0.34986301  -9.85013699  -4.87013699  -1.80013699\n[11]  -3.90013699  -0.85013699  -7.17013699  -7.30013699  -9.80013699\n[16]  -4.60013699   6.26986301   5.74986301  -3.48013699  -2.95013699\n[21]   4.09986301   1.31986301  -5.22013699  -9.17013699   6.67986301\n[26]   2.71986301  -4.07013699   6.84986301   5.46986301  -1.93013699\n[31]  10.34986301   0.76986301   4.89986301   5.54986301   8.49986301\n[36]  -1.90013699   5.94986301  -1.32013699   3.97986301   3.24986301\n[41]   3.34986301   5.46986301   5.34986301   0.07986301   0.07986301\n[46]   0.06986301  -0.03013699  -0.40013699  -9.40013699  -9.57013699\n[51]  -9.40013699 -10.40013699  -9.23013699   3.42986301   3.84986301\n[56]   4.42986301   4.34986301   3.59986301   6.84986301   6.84986301\n[61]  -8.07013699   9.34986301  -1.12013699  -1.58013699  -1.28013699\n[66]  -8.90013699  -9.78013699   9.67986301  11.42986301   9.84986301\n[71]  -8.78013699 -13.20013699   0.22986301\n\nc(scale(df$LAT, scale = F))\n\n [1]   6.29575342   7.21575342   5.67575342   3.84575342   6.79575342\n [6]  -1.23424658  -7.48424658  -3.15424658  -4.80424658   0.71575342\n[11]   7.64575342  -6.62424658  -6.77424658   5.22575342  -1.00424658\n[16]   1.44575342   3.62575342   4.14575342 -10.52424658   3.42575342\n[21]  -8.50424658  10.59575342  -7.13424658  -4.05424658   1.76575342\n[26]  -8.10424658   3.64575342  -1.93424658  -3.00424658   4.92575342\n[31]  -0.28424658   1.31575342  -1.58424658   0.06575342  -0.94424658\n[36]  -1.55424658  -4.83424658  -5.82424658   0.36575342   0.34575342\n[41]  -0.22424658  -0.18424658   0.01575342   5.71575342   5.76575342\n[46]   5.77575342   5.74575342   5.37575342   3.39575342  -9.52424658\n[51] -11.10424658  -6.35424658  -8.77424658  -0.44424658  -0.02424658\n[56]   0.39575342   0.47575342   0.39575342  -1.60424658  -1.60424658\n[61]  -9.85424658   0.22575342   1.01575342   1.14575342   1.96575342\n[66]   9.76575342   7.64575342  -3.27424658  -4.10424658  -3.43424658\n[71]  -1.35424658   5.29575342  12.02575342\n\n\nHowever, these columns are already present in the dataset as CLAT and CLONG.\n\nmodel3 &lt;- lm(LC3 ~ CLAT*CLONG, data = df)\nsummary(model3)\n\n\nCall:\nlm(formula = LC3 ~ CLAT * CLONG, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54185 -0.13298 -0.02287  0.16807  0.43410 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.5529416  0.0274679 -20.130  &lt; 2e-16 ***\nCLAT         0.0483954  0.0057047   8.483 2.61e-12 ***\nCLONG       -0.0025787  0.0043182  -0.597   0.5523    \nCLAT:CLONG   0.0022522  0.0008757   2.572   0.0123 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2334 on 69 degrees of freedom\nMultiple R-squared:  0.5137,    Adjusted R-squared:  0.4926 \nF-statistic:  24.3 on 3 and 69 DF,  p-value: 7.657e-11\n\n\nFinal equation for the model 3 (Interaction):\n\n\n\\[\n\\operatorname{LC3} = \\beta_{0} + \\beta_{1}(\\operatorname{CLAT}) + \\beta_{2}(\\operatorname{CLONG}) + \\beta_{3}(\\operatorname{CLAT} \\times \\operatorname{CLONG}) + \\epsilon\n\\]\n\n\n\\[\n\\operatorname{\\widehat{LC3}} = -0.55 + 0.05(\\operatorname{CLAT}) + 0(\\operatorname{CLONG}) + 0(\\operatorname{CLAT} \\times \\operatorname{CLONG})\n\\]\n\n\n\n# tolerance\n1/car::vif(model3)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n      CLAT      CLONG CLAT:CLONG \n 0.8268942  0.9799097  0.8195915 \n\n\nNow the collinearity problem has disappeared.\n\n\nDiagnostic plots\n\npar(mfrow = c(2,2))\nplot(model3)\n\n\n\n\nDiagnostic checks of the model did not reveal any outliers nor influential values.\n\n\nBoxplot of residuals\n\nboxplot(residuals(model3))\n\n\n\n\nThe boxplot of residuals was reasonably symmetrical and although there was some heterogeneity in spread of residuals when plotted against predicted values, and a \\(45°\\) line representing sites with zero abundance of \\(C_3\\) plants, this was not of a form that could be simply corrected (Figure 6.2).\n\n\nModel 4: stepwise selection\nStepwise selection is a method used in regression analysis to select the most significant predictors for a model. It involves adding or removing variables iteratively based on specific criteria, such as p-values, AIC (Akaike Information Criterion), or adjusted R². There are three main types: forward selection, which starts with no variables and adds them one by one; backward elimination, which starts with all variables and removes the least significant ones; and stepwise selection, which combines both forward and backward methods, adding or removing variables at each step. The goal is to find a balance between model simplicity and explanatory power.\nWe’ll use the function step(), with the argument:\n\ndirection = \"both\" for stepwise selection\ndirection = \"backward\" for backward selection\ndirection = \"forward\" for forward selection\n\nOut of interest, we also ran the full model with all six predictors through both a forward and backward selection routine for stepwise multiple regression. For both methods, the significance level for entering and removing terms based on partial F statistics was set at 0.15.\n\n\nBackward selection\nThe backward selection is as follows.\n\n\n\nCoefficient\nEstimate\nStandard error\nt\nP-value\n\n\n\n\nJJAMAP\n-1.002\n0.433\n-2.314\n0.024\n\n\nDJFMAP\n-1.005\n0.485\n-2.070\n0.042\n\n\nLAT\n0.042\n0.005\n8.033\n&lt;0.001\n\n\n\n\nlibrary(MASS)\n# we use an additive model with 6 explicative variables\nmodel4 &lt;- lm(LC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT, data = df)\n# we apply the backward selection\nmodel4 &lt;- step(model4, direction = \"backward\", test = \"F\")\n\nStart:  AIC=-202.51\nLC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT\n\n         Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n- MAT     1   0.00030 3.7611 -204.50  0.0053   0.94215    \n- LONG    1   0.02714 3.7879 -203.98  0.4762   0.49256    \n- MAP     1   0.09065 3.8514 -202.77  1.5908   0.21165    \n- DJFMAP  1   0.10274 3.8635 -202.54  1.8030   0.18395    \n&lt;none&gt;                3.7608 -202.51                      \n- JJAMAP  1   0.17554 3.9363 -201.18  3.0807   0.08387 .  \n- LAT     1   1.09047 4.8513 -185.92 19.1371 4.424e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=-204.5\nLC3 ~ MAP + JJAMAP + DJFMAP + LONG + LAT\n\n         Df Sum of Sq    RSS     AIC F value   Pr(&gt;F)    \n- LONG    1    0.0282 3.7893 -205.96  0.5019  0.48111    \n- MAP     1    0.0904 3.8515 -204.76  1.6108  0.20877    \n- DJFMAP  1    0.1030 3.8641 -204.53  1.8346  0.18014    \n&lt;none&gt;                3.7611 -204.50                     \n- JJAMAP  1    0.1756 3.9367 -203.17  3.1280  0.08151 .  \n- LAT     1    3.5320 7.2931 -158.16 62.9183 3.19e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=-205.95\nLC3 ~ MAP + JJAMAP + DJFMAP + LAT\n\n         Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n- MAP     1    0.0643 3.8535 -206.73  1.1535   0.28661    \n- DJFMAP  1    0.0753 3.8646 -206.52  1.3517   0.24905    \n&lt;none&gt;                3.7893 -205.96                      \n- JJAMAP  1    0.1682 3.9575 -204.78  3.0192   0.08681 .  \n- LAT     1    3.5512 7.3404 -159.69 63.7272 2.353e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=-206.73\nLC3 ~ JJAMAP + DJFMAP + LAT\n\n         Df Sum of Sq    RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                3.8535 -206.73                      \n- DJFMAP  1    0.2393 4.0929 -204.33  4.2856   0.04218 *  \n- JJAMAP  1    0.2990 4.1526 -203.27  5.3542   0.02366 *  \n- LAT     1    3.6041 7.4577 -160.53 64.5343 1.737e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model4)\n\n\nCall:\nlm(formula = LC3 ~ JJAMAP + DJFMAP + LAT, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55475 -0.16509  0.02634  0.18400  0.48787 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.724723   0.305825  -5.640 3.46e-07 ***\nJJAMAP      -1.001984   0.433027  -2.314   0.0237 *  \nDJFMAP      -1.005341   0.485632  -2.070   0.0422 *  \nLAT          0.042309   0.005267   8.033 1.74e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2363 on 69 degrees of freedom\nMultiple R-squared:  0.5016,    Adjusted R-squared:  0.4799 \nF-statistic: 23.15 on 3 and 69 DF,  p-value: 1.775e-10\n\n\nFinal equation for the model 4 (Backward):\n\n\n\\[\n\\operatorname{LC3} = \\beta_{0} + \\beta_{1}(\\operatorname{JJAMAP}) + \\beta_{2}(\\operatorname{DJFMAP}) + \\beta_{3}(\\operatorname{LAT}) + \\epsilon\n\\]\n\n\n\\[\n\\operatorname{\\widehat{LC3}} = -1.72 - 1(\\operatorname{JJAMAP}) - 1.01(\\operatorname{DJFMAP}) + 0.04(\\operatorname{LAT})\n\\]\n\n\nThe forward selection is as follows.\n\n\n\nCoefficient\nEstimate\nStandard error\nt\nP-value\n\n\n\n\nMAP\n&lt;0.001\n&lt;0.001\n1.840\n0.070\n\n\nLAT\n0.044\n0.005\n66.319\n&lt;0.001\n\n\n\n\nlibrary(MASS)\n# we use an additive model with 6 explicative variables\nmodel4 &lt;- lm(LC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT, data = df)\n# we apply the forward selection\nmodel4 &lt;- step(model4, direction = \"forward\", test = \"F\")\n\nStart:  AIC=-202.51\nLC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT\n\nsummary(model4)\n\n\nCall:\nlm(formula = LC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT, \n    data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55607 -0.17084  0.02938  0.18397  0.47953 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.6886133  1.2391496  -2.170   0.0336 *  \nMAP          0.0002743  0.0002175   1.261   0.2117    \nMAT         -0.0008468  0.0116261  -0.073   0.9422    \nJJAMAP      -0.8338536  0.4750796  -1.755   0.0839 .  \nDJFMAP      -0.9618361  0.7163073  -1.343   0.1839    \nLONG         0.0069236  0.0100330   0.690   0.4926    \nLAT          0.0434331  0.0099285   4.375 4.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2387 on 66 degrees of freedom\nMultiple R-squared:  0.5136,    Adjusted R-squared:  0.4694 \nF-statistic: 11.61 on 6 and 66 DF,  p-value: 7.787e-09\n\n\nFinal equation for the model 4 (Forward):\n\n\n\\[\n\\operatorname{LC3} = \\beta_{0} + \\beta_{1}(\\operatorname{MAP}) + \\beta_{2}(\\operatorname{MAT}) + \\beta_{3}(\\operatorname{JJAMAP}) + \\beta_{4}(\\operatorname{DJFMAP}) + \\beta_{5}(\\operatorname{LONG}) + \\beta_{6}(\\operatorname{LAT}) + \\epsilon\n\\]\n\n\n\\[\n\\operatorname{\\widehat{LC3}} = -2.69 + 0(\\operatorname{MAP}) + 0(\\operatorname{MAT}) - 0.83(\\operatorname{JJAMAP}) - 0.96(\\operatorname{DJFMAP}) + 0.01(\\operatorname{LONG}) + 0.04(\\operatorname{LAT})\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe results in R differ from those in the book because it used different metrics to select the best model. The book uses an F-test set at 0.15.\n\n\n\n\nMetrics AIC and BIC for variable selection\nAkaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are statistical measures used for model selection, particularly in the context of regression and other predictive models. Both criteria penalize model complexity to prevent overfitting, but they do so differently: AIC is based on the concept of information loss, favoring models that minimize the information loss between the true model and the approximating model, while BIC incorporates a stronger penalty for the number of parameters as the sample size increases, making it more conservative in terms of model complexity. Specifically, AIC is calculated as\nAIC = \\(-2log(L) + 2k\\) (where (\\(L\\)) is the likelihood of the model and (\\(k\\)) is the number of parameters), whereas BIC is computed as\nBIC = \\(-2log(L) + k \\space log(n)\\) (where (\\(n\\)) is the sample size).\nIn practice, lower values of AIC or BIC indicate a better-fitting model, with AIC often used for predictive performance and BIC more frequently used for model selection when the goal is to identify the true model structure.\n\n\nModel 5: AIC for selecting the best model\n\nmodel5 &lt;- lm(LC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT, data = df)\n# we select k = 2 to represent AIC \nmodel5_AIC &lt;- step(model5, direction = \"backward\", k = 2)\n\nStart:  AIC=-202.51\nLC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT\n\n         Df Sum of Sq    RSS     AIC\n- MAT     1   0.00030 3.7611 -204.50\n- LONG    1   0.02714 3.7879 -203.98\n- MAP     1   0.09065 3.8514 -202.77\n- DJFMAP  1   0.10274 3.8635 -202.54\n&lt;none&gt;                3.7608 -202.51\n- JJAMAP  1   0.17554 3.9363 -201.18\n- LAT     1   1.09047 4.8513 -185.92\n\nStep:  AIC=-204.5\nLC3 ~ MAP + JJAMAP + DJFMAP + LONG + LAT\n\n         Df Sum of Sq    RSS     AIC\n- LONG    1    0.0282 3.7893 -205.96\n- MAP     1    0.0904 3.8515 -204.76\n- DJFMAP  1    0.1030 3.8641 -204.53\n&lt;none&gt;                3.7611 -204.50\n- JJAMAP  1    0.1756 3.9367 -203.17\n- LAT     1    3.5320 7.2931 -158.16\n\nStep:  AIC=-205.95\nLC3 ~ MAP + JJAMAP + DJFMAP + LAT\n\n         Df Sum of Sq    RSS     AIC\n- MAP     1    0.0643 3.8535 -206.73\n- DJFMAP  1    0.0753 3.8646 -206.52\n&lt;none&gt;                3.7893 -205.96\n- JJAMAP  1    0.1682 3.9575 -204.78\n- LAT     1    3.5512 7.3404 -159.69\n\nStep:  AIC=-206.73\nLC3 ~ JJAMAP + DJFMAP + LAT\n\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                3.8535 -206.73\n- DJFMAP  1    0.2393 4.0929 -204.33\n- JJAMAP  1    0.2990 4.1526 -203.27\n- LAT     1    3.6041 7.4577 -160.53\n\n# final model with the best AIC\nsummary(model5_AIC)\n\n\nCall:\nlm(formula = LC3 ~ JJAMAP + DJFMAP + LAT, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55475 -0.16509  0.02634  0.18400  0.48787 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.724723   0.305825  -5.640 3.46e-07 ***\nJJAMAP      -1.001984   0.433027  -2.314   0.0237 *  \nDJFMAP      -1.005341   0.485632  -2.070   0.0422 *  \nLAT          0.042309   0.005267   8.033 1.74e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2363 on 69 degrees of freedom\nMultiple R-squared:  0.5016,    Adjusted R-squared:  0.4799 \nF-statistic: 23.15 on 3 and 69 DF,  p-value: 1.775e-10\n\n\nFinal equation for the model 5 (AIC - backward):\n\n\n\\[\n\\operatorname{LC3} = \\beta_{0} + \\beta_{1}(\\operatorname{JJAMAP}) + \\beta_{2}(\\operatorname{DJFMAP}) + \\beta_{3}(\\operatorname{LAT}) + \\epsilon\n\\]\n\n\n\\[\n\\operatorname{\\widehat{LC3}} = -1.72 - 1(\\operatorname{JJAMAP}) - 1.01(\\operatorname{DJFMAP}) + 0.04(\\operatorname{LAT})\n\\]\n\n\n\nApplying BIC for selecting the best model\n\nmodel5 &lt;- lm(LC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT, data = df)\n# we apply k = log of the number of observations to represent BIC\n# n is the number of observations in your dataset\nn &lt;- nrow(df)\nmodel5_BIC &lt;- step(model5, direction = \"backward\", k = log(n))\n\nStart:  AIC=-186.47\nLC3 ~ MAP + MAT + JJAMAP + DJFMAP + LONG + LAT\n\n         Df Sum of Sq    RSS     AIC\n- MAT     1   0.00030 3.7611 -190.76\n- LONG    1   0.02714 3.7879 -190.24\n- MAP     1   0.09065 3.8514 -189.02\n- DJFMAP  1   0.10274 3.8635 -188.79\n- JJAMAP  1   0.17554 3.9363 -187.43\n&lt;none&gt;                3.7608 -186.47\n- LAT     1   1.09047 4.8513 -172.18\n\nStep:  AIC=-190.76\nLC3 ~ MAP + JJAMAP + DJFMAP + LONG + LAT\n\n         Df Sum of Sq    RSS     AIC\n- LONG    1    0.0282 3.7893 -194.50\n- MAP     1    0.0904 3.8515 -193.31\n- DJFMAP  1    0.1030 3.8641 -193.08\n- JJAMAP  1    0.1756 3.9367 -191.72\n&lt;none&gt;                3.7611 -190.76\n- LAT     1    3.5320 7.2931 -146.71\n\nStep:  AIC=-194.5\nLC3 ~ MAP + JJAMAP + DJFMAP + LAT\n\n         Df Sum of Sq    RSS     AIC\n- MAP     1    0.0643 3.8535 -197.56\n- DJFMAP  1    0.0753 3.8646 -197.36\n- JJAMAP  1    0.1682 3.9575 -195.62\n&lt;none&gt;                3.7893 -194.50\n- LAT     1    3.5512 7.3404 -150.52\n\nStep:  AIC=-197.57\nLC3 ~ JJAMAP + DJFMAP + LAT\n\n         Df Sum of Sq    RSS     AIC\n&lt;none&gt;                3.8535 -197.56\n- DJFMAP  1    0.2393 4.0929 -197.46\n- JJAMAP  1    0.2990 4.1526 -196.40\n- LAT     1    3.6041 7.4577 -153.66\n\n# final model with the best BIC\nsummary(model5_BIC)\n\n\nCall:\nlm(formula = LC3 ~ JJAMAP + DJFMAP + LAT, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55475 -0.16509  0.02634  0.18400  0.48787 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.724723   0.305825  -5.640 3.46e-07 ***\nJJAMAP      -1.001984   0.433027  -2.314   0.0237 *  \nDJFMAP      -1.005341   0.485632  -2.070   0.0422 *  \nLAT          0.042309   0.005267   8.033 1.74e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2363 on 69 degrees of freedom\nMultiple R-squared:  0.5016,    Adjusted R-squared:  0.4799 \nF-statistic: 23.15 on 3 and 69 DF,  p-value: 1.775e-10\n\n\nFinal equation for the model 5 (BIC):\n\n\n\\[\n\\operatorname{LC3} = \\beta_{0} + \\beta_{1}(\\operatorname{JJAMAP}) + \\beta_{2}(\\operatorname{DJFMAP}) + \\beta_{3}(\\operatorname{LAT}) + \\epsilon\n\\]\n\n\n\\[\n\\operatorname{\\widehat{LC3}} = -1.72 - 1(\\operatorname{JJAMAP}) - 1.01(\\operatorname{DJFMAP}) + 0.04(\\operatorname{LAT})\n\\]\n\n\n\n\n\nWhen to use AIC or BIC\n\nUse AIC for better predictive accuracy, especially when working with smaller datasets or when the true model is likely complex.\nUse BIC for simpler, more parsimonious models, especially when you have a large dataset and want to avoid overfitting.\n\n\n\nImportance of RMSE\nThe Root Mean Squared Error (RMSE) is a key metric for evaluating the performance of predictive models, especially in regression analysis. It provides a measure of how well a model’s predictions match the actual observed data. Here are some reasons why RMSE is important in evaluating models:\n\n1. Measures Prediction Accuracy:\nRMSE gives a direct indication of how close the predicted values are to the actual values in the dataset. Lower RMSE values indicate better fit, as they suggest that the predictions are closer to the real observations.\n\n\n2. Interpretability:\nRMSE is expressed in the same units as the dependent variable (response variable), making it easy to interpret in the context of the problem. For example, if you’re predicting house prices in dollars, RMSE will tell you the average prediction error in dollars, which makes it more intuitive.\n\n\n3. Sensitive to Large Errors:\nRMSE gives more weight to large errors due to the squaring of the residuals before averaging. This makes RMSE a good choice when you want to penalize models that make large prediction errors and want to emphasize minimizing significant deviations.\n\n\n4. Comparison of Models:\nRMSE is often used to compare different models’ performances. The model with the lowest RMSE is usually considered the better model when comparing across models with similar complexity and scale. It helps in choosing between competing models during model selection.\n\n\n5. Standard Model Performance Metric:\nRMSE is one of the most commonly used metrics in machine learning and statistical modeling. It provides a standard way to evaluate models across different domains and applications.\n\n\nLimitations of RMSE:\n\nSensitive to Scale: If your dependent variable has a wide range, RMSE values can be misleading. In such cases, normalized error metrics like MAE (Mean Absolute Error) might complement RMSE.\nDoesn’t Work Well with Outliers: Since RMSE squares the errors, it can be disproportionately affected by outliers, making it essential to consider this when choosing it as a performance measure.\n\nIn summary, RMSE is important because it gives a clear, interpretable, and meaningful measure of prediction accuracy and helps guide model comparison and selection based on how well a model can predict unseen data.\n\nsqrt(mean(model5_BIC$residuals^2))\n\n[1] 0.229757"
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Correlation and linear regressions (4.1 and 4.2)",
    "section": "",
    "text": "Example: crab and burrow density on Christmas Island (p. 73)\n\n\n\nGreen (1997) studied the ecology of red land crabs on Christmas Island and examined the relationship between the total biomass of red land crabs and the density of their burrows within 25 m2 quadrats (sampling units) at five forested sites on the island.We will look at two of these sites: there were ten quadrats at Lower Site (LS) and eight quadrats at Drumsite (DS). Scatterplots and boxplots are presented in Figure 5.3. There was slight negative skewness for biomass and burrow density for LS, and an outlier for burrow density for DS, but no evidence of nonlinearity. Pearson’s correlation coefficient was considered appropriate for these data although more robust correlations were calculated for comparison.\n\n\n\nSite\nCorrelation type\nStatistic\nP-value\n\n\n\n\nDS (n = 8)\nPearson\n0.392\n0.337\n\n\n\nSpearman\n0.168\n0.691\n\n\n\nKendall\n0.036\n0.901\n\n\nLS (n = 10)\nPearson\n0.882\n0.001\n\n\n\nSpearman\n0.851\n0.002\n\n\n\nKendall\n0.719\n0.004\n\n\n\nThe \\(H_0\\) of no linear relationship between total crab biomass and number of burrows at DS could not be rejected. The same conclusion applies for monotonic relationships measured by Spearman and Kendall’s coefficients. So there was no evidence for any linear or more general monotonic relationship between burrow density and total crab biomass at site DS.\nThe \\(H_0\\) of no linear relationship between total crab biomass and number of burrows at LS was rejected. The same conclusion applies for monotonic relationships measured by Spearman and Kendall’s coefficients. There was strong evidence of a linear and more general monotonic relationship between burrow density and total crab biomass at site LS.\n\n\n\n# get the data from the .csv file\ndf &lt;- read.csv(\"data/green.csv\")\n\nThe data we’ll use for this analysis:\n\nDT::datatable(df)\n\n\n\n\n\n\n\nPlot the data\nPlotting the data before performing analysis is crucial because it allows you to visually assess key characteristics and potential issues in the dataset. Here’s why it’s important:\n\nIdentify Patterns: Visualization helps you detect relationships between variables (e.g., linear, non-linear, or no relationship) that might guide your choice of analysis techniques.\nSpot Outliers: You can easily notice any extreme values or outliers that might distort the analysis or require further investigation.\nCheck for Assumptions: Visualizing the data allows you to see if assumptions for statistical methods (e.g., normality, homoscedasticity, or linearity) are likely to be met.\nDetect Data Errors: Unusual patterns or values in the plot can help you identify errors or anomalies in the data, such as incorrect entries or missing values.\nChoose the Right Model: By plotting the data, you can gain insights into whether a linear model is appropriate or if other transformations or models (e.g., polynomial or logarithmic) are needed.\n\nOverall, plotting the data gives you an initial understanding and helps prevent misinterpretations that could arise from relying solely on numerical methods.\n\nlibrary(ggplot2)\nggplot(df, aes(x = TOTMASS, y = BURROWS)) +\n  geom_point() +\n  facet_wrap(SITE~.)\n\n\n\n\n\n\nPearson’s correlation between total biomass and burrows at Drumsite (DS)\n\n\n\n\n\n\nseparate_df &lt;- subset(df, SITE == \"DS\")\ncor.test(formula = ~ TOTMASS + BURROWS, data = separate_df)\n\n\n    Pearson's product-moment correlation\n\ndata:  TOTMASS and BURROWS\nt = 1.0428, df = 6, p-value = 0.3372\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.4322803  0.8592175\nsample estimates:\n      cor \n0.3917155 \n\n\n\n\n\n\n\n\nINTERPRETATION\n\n\n\nThe Pearson’s correlation between TOTMASS and BURROWS (Site = DS) is 0.39 (p-value = 0.33). The 95% confidence interval is from -0.4 to 0.85.\n\n\n\n\nPearson’s correlation between total biomass and burrows at Lower Site (LS)\n\nseparate_df &lt;- subset(df, SITE == \"LS\")\ncor.test(formula = ~ TOTMASS + BURROWS, data = separate_df)\n\n\n    Pearson's product-moment correlation\n\ndata:  TOTMASS and BURROWS\nt = 5.2925, df = 8, p-value = 0.000735\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5674158 0.9718892\nsample estimates:\n      cor \n0.8819549 \n\n\n\n\n\n\n\n\nINTERPRETATION\n\n\n\nThe Pearson’s correlation between TOTMASS and BURROWS (Site = LS) is 0.88 (p-value &lt; 0.05). The 95% confidence interval is from 0.56 to 0.97.\n\n\n\n\n\n\n\n\nDifferent importances\n\n\n\nThese interpretations (e.g., strong, weak, etc.) reflect the overall significance of the correlations. However, the importance of these correlations varies across different scientific fields. A correlation of 0.5 may be weak for engineers , but it can be strong for biologists, for example.\n\n\n\n\n\n\n\n\nCorrelation ≠ Causation\n\n\n\nJust because two things are related doesn’t mean one causes the other. Correlation shows a connection, but it doesn’t prove causation! Check this interesting article.\n\n\n\n\nLinear models\n\nModelling burrows as function of total biomass at Lower Site (LS)\n\nVariable \\(X\\) (predictor): total biomass\n\nVariable \\(Y\\) (response): burrows\n\n\nseparate_df &lt;- subset(df, SITE == \"LS\")\nmodel1 &lt;- lm(data = df, BURROWS ~ TOTMASS)\nsummary(model1)\n\n\nCall:\nlm(formula = BURROWS ~ TOTMASS, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.054  -8.737  -0.863   1.297  40.970 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  36.7991    11.2461   3.272  0.00479 **\nTOTMASS       0.4771     3.3415   0.143  0.88824   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.08 on 16 degrees of freedom\nMultiple R-squared:  0.001273,  Adjusted R-squared:  -0.06115 \nF-statistic: 0.02039 on 1 and 16 DF,  p-value: 0.8882\n\npar(mfrow = c(2,2))\nplot(model1)\n\n\n\n\n\n\nFitted regression line of the model\n\nplot(BURROWS ~ TOTMASS, data =  separate_df)\n#add fitted regression line to scatterplot\nabline(model1)\n\n\n\n\n\n\nModelling burrows as a function of total biomass at Drumsite (DS)\n\nVariable \\(X\\) (predictor): total biomass\n\nVariable \\(Y\\) (response): burrows\n\n\nseparate_df &lt;- subset(df, SITE == \"DS\")\nmodel2 &lt;- lm(data = separate_df, BURROWS ~ TOTMASS)\nsummary(model2)\n\n\nCall:\nlm(formula = BURROWS ~ TOTMASS, data = separate_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.871  -7.926  -2.886   3.321  32.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   27.465     18.247   1.505    0.183\nTOTMASS        7.247      6.949   1.043    0.337\n\nResidual standard error: 16.46 on 6 degrees of freedom\nMultiple R-squared:  0.1534,    Adjusted R-squared:  0.01235 \nF-statistic: 1.088 on 1 and 6 DF,  p-value: 0.3372\n\nplot(model2, which = 1)\n\n\n\n\n\n\nFitted regression line of the model\n\nplot(BURROWS ~ TOTMASS, data =  separate_df)\n#add fitted regression line to scatterplot\nabline(model2)"
  },
  {
    "objectID": "anova_repeated_measures.html",
    "href": "anova_repeated_measures.html",
    "title": "One-way repeated measures ANOVA",
    "section": "",
    "text": "A one-way repeated measures ANOVA is used when you want to test for differences between three or more related groups (conditions, time points, etc.), where the same subjects or experimental units are measured multiple times under different conditions or over time. This type of ANOVA takes into account the fact that the measurements are not independent because they come from the same subjects.\n\nKey Concepts of One-Way Repeated Measures ANOVA:\n\nOne-way: Refers to having only one factor (independent variable) with multiple levels (e.g., different time points, conditions, or treatments).\nRepeated measures: Means that the same subjects (or experimental units) are measured multiple times, under each level of the independent variable.\nWithin-subject design: Since measurements are taken on the same subject, the variability due to individual differences is controlled, making the analysis more sensitive to detecting differences across conditions.\n\n\n\nAssumptions:\n\nSphericity: The variances of the differences between all combinations of related groups should be equal. This is a key assumption for repeated measures ANOVA. Violations of sphericity can be tested using Mauchly’s test.\nNormality: The dependent variable should be approximately normally distributed within each level of the repeated measure factor.\nIndependence: Although measurements are repeated, observations within a group (e.g., different subjects) should still be independent of each other.\n\n\n\n\n\n\n\nNote\n\n\n\nData we’ll use: driscoll1.csv (chap 10), boîte 10.2 p.266 Quinn & Keough\n\n\n\n# get the data\ndf &lt;- read.csv(\"data/driscoll1.csv\")\n\n\n# view the data\nDT::datatable(df)\n\n\n\n\n\n\n\n\nPreparing the data\nThe data has a wrong format. The years should be a categorical column\n\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(rstatix)\n\ndf$id &lt;- c(1:6)\ndf &lt;- pivot_longer(df,\n                   cols = 1:3,\n                   names_to = \"year\",\n                   values_to = \"number\")\ndf$year &lt;- as.factor(df$year)\n\n\n\nNormality assumption\n\nggqqplot(df, \"number\", facet.by = \"year\")\n\n\n\n\nFrom the plot above, as all the points fall approximately along the reference line, we can assume normality, apart from 1 point.\n\n\nVisualization\n\nggboxplot(df, x = \"year\", y = \"number\", add = \"point\")\n\n\n\n\n\n\nTesting for Sphericity: Mauchly’s Test of Sphericity\n\nmodel &lt;- anova_test(data = df, dv = number, wid = id, within = year)\nmodel$ANOVA\n\n  Effect DFn DFd    F     p p&lt;.05   ges\n1   year   2  10 9.66 0.005     * 0.244\n\nmodel$`Mauchly's Test for Sphericity`\n\n  Effect     W     p p&lt;.05\n1   year 0.596 0.355      \n\nmodel$`Sphericity Corrections`\n\n  Effect   GGe     DF[GG] p[GG] p[GG]&lt;.05   HFe     DF[HF] p[HF] p[HF]&lt;.05\n1   year 0.712 1.42, 7.12 0.013         * 0.915 1.83, 9.15 0.006         *"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "1MRR823 - Analyses quantitatives dans le domaine des ressources renouvelables",
    "section": "",
    "text": "Welcome to the resource hub for the course 1MRR823! Here, you’ll find a collection of R code snippets designed to enhance your learning experience. These codes are tailored to support the concepts covered in class and help you apply your knowledge effectively. Dive in and explore!\nYou can use the links on the left side to choose your subject."
  },
  {
    "objectID": "non-parametric.html",
    "href": "non-parametric.html",
    "title": "Types of correlation (4.3)",
    "section": "",
    "text": "It is necessary to verify some aspects of the data in order to check the correlation between the variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspect\nPearson\nSpearman\nKendall\n\n\n\n\nType of relationship\nLinear\nMonotonic (any shape)\nMonotonic (any shape)\n\n\nData assumptions\nContinuous, normal, no outliers\nOrdinal or continuous, any distribution, fewer assumptions\nOrdinal or continuous, handles tied ranks well\n\n\nEffect of outliers\nSensitive to outliers\nLess sensitive\nLeast sensitive\n\n\nRange\n-1 to 1\n-1 to 1\n-1 to 1\n\n\nUse case\nLinear relationships, normal data\nNon-linear, monotonic relationships, or ordinal data\nSmall datasets, ordinal data, or data with ties\n\n\n\nWe can use the same function in R for all these correlations. In the function cor.test(), we just need to change the parameter method to \"pearson\", \"spearman\" or \"kendall\".\n\nExample\ncor.test(formula = ~ VARIABLE_1 + VARIABLE_2, data = DATASET, method = \"pearson\")\ncor.test(formula = ~ VARIABLE_1 + VARIABLE_2, data = DATASET, method = \"kendall\")\ncor.test(formula = ~ VARIABLE_1 + VARIABLE_2, data = DATASET, method = \"spearman\")\n\n# get the data\ndf &lt;- read.csv(\"data/green.csv\")\n\n\n\nPearson’s correlation for site DS\n\n# separate the sites\ndf_siteDS &lt;- subset(df, SITE == \"DS\")\n# get the correlation\ncor.test(formula = ~ TOTMASS + BURROWS, data = df_siteDS, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  TOTMASS and BURROWS\nt = 1.0428, df = 6, p-value = 0.3372\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.4322803  0.8592175\nsample estimates:\n      cor \n0.3917155 \n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nModerate Positive Correlation: The Pearson correlation coefficient of approximately 0.3917 indicates a moderate positive correlation between TOTMASS and BURROWS, suggesting that as one variable increases, the other tends to increase as well.\nNot Statistically Significant: The p-value of 0.3372 indicates that this correlation is not statistically significant, meaning that it could easily arise due to random chance in this sample.\nConfidence Interval: The confidence interval ranging from -0.4323 to 0.8592 includes zero, reinforcing the idea that there is insufficient evidence to conclude a true correlation exists in the population.\n\n\n\n\nSpearman’s correlation for site DS\nThis correlation uses the Greek letter \\(\\rho\\) (Rho).\n\n# separate the sites\ndf_siteDS &lt;- subset(df, SITE == \"DS\")\n# get the correlation\ncor.test(formula = ~ TOTMASS + BURROWS, data = df_siteDS, method = \"spearman\")\n\nWarning in cor.test.default(x = mf[[1L]], y = mf[[2L]], ...): Cannot compute\nexact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  TOTMASS and BURROWS\nS = 69.916, p-value = 0.6915\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.1676677 \n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nWeak Correlation: The correlation coefficient (rho = 0.1677) suggests a weak positive correlation between TOTMASS and BURROWS.\nNot Significant: The high p-value (0.6915) indicates that this weak correlation is not statistically significant, meaning that there’s no strong evidence of a monotonic relationship between the variables in the sample.\nTies in Data: The presence of ties in the data affects the precision of the p-value calculation, but it does not change the interpretation that the relationship between TOTMASS and BURROWS is likely weak and non-significant in this case.\n\n\n\n\nKendall’s correlation for site DS\nThis correlation uses the Greek letter \\(\\tau\\) (tau).\n\n# separate the sites\ndf_siteDS &lt;- subset(df, SITE == \"DS\")\n# get the correlation\ncor.test(formula = ~ TOTMASS + BURROWS, data = df_siteDS, method = \"kendall\")\n\nWarning in cor.test.default(x = mf[[1L]], y = mf[[2L]], ...): Cannot compute\nexact p-value with ties\n\n\n\n    Kendall's rank correlation tau\n\ndata:  TOTMASS and BURROWS\nz = 0.12468, p-value = 0.9008\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n0.03636965 \n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nVery Weak Correlation: The tau value (0.03637) indicates a very weak positive correlation, suggesting that TOTMASS and BURROWS do not have a meaningful monotonic relationship.\nNot Significant: The high p-value (0.9008) shows that the weak correlation is not statistically significant, leading to the conclusion that you cannot reject the null hypothesis that the true Kendall correlation is zero.\nTies in Data: As with the Spearman correlation, the presence of ties impacts the precision of the p-value calculation, but the interpretation remains consistent: the relationship between TOTMASS and BURROWS is weak and not significant."
  },
  {
    "objectID": "simple_linear_regression.html",
    "href": "simple_linear_regression.html",
    "title": "Simple Linear Regression (5.1, 5.2)",
    "section": "",
    "text": "Example from the book Experimental Design and Data Analysis for Biologists by Gerry P. Quinn and Michael J. Keough\nPage 81, Chapter 5 - Correlation and regression.\n\n\n\n\n\n\nBox 5.4 - Worked example of linear regression analysis: species richness of macroinvertebrates in mussel clumps.\n\n\n\nPeake & Quinn (1993) investigated the relationship between the number of species of macroinvertebrates, and the total abundance of macroinvertebrates, and area of clumps of mussels on a rocky shore in southern Australia. The variables of interest are clump area (dm\\(^2\\)), number of species, and number of individuals.\n\nNumber of species against clump area\nA scatterplot of number of species against clump area, and the plot of residuals against predicted number of species from a linear regression analysis,both suggested a nonlinear relationship (Figure 5.17(a,b)). Although only clump area was positively skewed, Peake & Quinn (1993) transformed both variables because of the nature of the species–area relationships for other seasons in their study plus the convention in species–area studies to transform both variables. The scatterplot of log number of species against log clump area (Figure 5.18) linearized the relationship effectively except for one of the small clumps. The residual plot also showed no evidence of nonlinearity but that same clump had a larger residual and was relatively influential (Cook’s D\\(_i\\) = 1.02). Reexamination of the raw data did not indicate any problems with this observation and omitting it did not alter the conclusions from the analysis (\\(\\beta_1\\) changed from 0.386 to 0.339, r\\(^2\\) from 0.819 to 0.850, all tests still P &lt; 0.001) so it was not excluded from the analysis. In fact, just transforming clump area produced the best linearizing of the relationship with no unusually large residuals or Cook’s D\\(_i\\) statistics but, for the reasons outlined above, both variables were transformed. The results of the OLS fit of a linear regression model to log number of species and log clump area were as follows.\nThe results of the OLS fit of a linear regression model to log number of species and log clump area were as follows.\nThe \\(t\\) test and the ANOVA \\(F\\) test cause us to reject the \\(H_0\\) that \\(\\beta_1\\) equals zero. We would also reject the \\(H_0\\) that \\(\\beta_0\\) equals zero, indicating that the relationship between species number and clump area must be nonlinear for small clump sizes since the model must theoretically go through the origin. The \\(r^2\\) value (0.819) indicates that we can explain about 82% of the total variation in log number of species by the linear regression with log clump area.\n\n\nNumber of individuals against clump area\nA scatterplot of number of individuals against clump area, with a Loess smoother fitted, suggested an approximately linear relationship (Figure 5.19(a)). The plot of residuals against predicted number of individuals from a linear regression model fitted to number of individuals against clump area (Figure 5.19(b)) showed a clear pattern of increasing spread of residuals against increasing predicted number of individuals (or, equivalently, clump area); the pattern in the residuals was wedge-shaped. The boxplots in Figure 5.19(a) indicated that both variables were positively skewed so we transformed both variables to logs to correct for variance heterogeneity. The scatterplot of log number of individuals against log clump area (Figure 5.20(a)) showed an apparent reasonable fit of a linear regression model, with symmetrical boxplots for both variables. The residual plot showed a more even spread of residuals with little wedge-shaped pattern (Figure 5.20(b)). The results of the OLS fit of a linear regression model to log number of individuals and log clump area were as follows.\nThe \\(t\\) test and the ANOVA \\(F\\) test cause us to reject the \\(H_0\\) that \\(\\beta_1\\) equals zero. We would also reject the \\(H_0\\) that \\(\\beta_0\\) equals zero, although this test is of little biological interest. The \\(r^2\\) value (0.859) indicates that we can explain about 86% of the total variation in log number of individuals by the linear regression with log clump area.\n\n\n\n\nlibrary(ggplot2)\n\n\n# get thedata\ndf &lt;- read.csv(\"data/peake.csv\")\n\n\nDT::datatable(df)\n\n\n\n\n\n\n\nA first look at the data\nWe want to adjust the number of species (Y) based on its area (X).\n\nggplot(data = df, aes(x = AREA, y = SPECIES)) +\n  geom_point()\n\n\n\n\n\n\nLinear regression\nWe use the function lm() to adjust the linear regression model. We use the following formula: Y ~ X.\n\nmodel &lt;- lm(SPECIES ~ AREA, data = df)\n\n\n\nMain info about the model\n\nsummary(model)\n\n\nCall:\nlm(formula = SPECIES ~ AREA, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1964 -2.7521 -0.7509  1.2094  7.2148 \n\nCoefficients:\n              Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 9.85616668 1.04396038   9.441 0.00000000224 ***\nAREA        0.00065930 0.00009283   7.102 0.00000031024 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.759 on 23 degrees of freedom\nMultiple R-squared:  0.6868,    Adjusted R-squared:  0.6732 \nF-statistic: 50.44 on 1 and 23 DF,  p-value: 0.0000003102\n\n\n\n\n\n\n\n\nWe obtained:\n\n\n\n\nEstimate: The estimated coefficients for each predictor, including the intercept. These show how much the dependent variable changes for a one-unit change in the predictor.\nStd. Error: The standard error of the estimated coefficients, indicating the variability in the coefficient estimates.\nt-value: The t-statistic for testing whether the coefficient is significantly different from zero. It is calculated as the Estimate divided by the Std. Error.\nPr(&gt;|t|): The p-value corresponding to the t-statistic, which indicates whether the predictor is statistically significant. A small p-value (e.g., &lt; 0.05) suggests the predictor significantly contributes to the model.\n\n\n\n\n\nPlot of the regression model\n\nplot1 &lt;- ggplot(df, aes(x = AREA, y = SPECIES)) +\n  geom_point() +                        # Scatter plot\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")  \nplot1\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nResiduals vs. fitted data (non transformed data)\n\nplot(model, which = 1)\n\n\n\n\nThere shouldn’t be a large variation of the residuals. We will change the scale of the data. Let’s take a look at the data distributions first using histograms.\n\n# histogram of area\nhist(df$AREA)\n\n\n\n# quantiles of area\nsummary(df$AREA)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  462.2  1773.7  4451.7  7802.0  9287.7 27144.0 \n\n\n\n# histogram of species\nhist(df$SPECIES)\n\n\n\n# quantiles of species\nsummary(df$SPECIES)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      3      10      14      15      21      25 \n\n\n\n\nWhen to apply the log transformation:\n\nWhen your residuals show non-constant variance (heteroscedasticity) or skewness.\nWhen the relationship between variables seems non-linear but multiplicative.\nWhen your variables contain large ranges of values or outliers that affect the model.\n\nWe could have transformed the data into \\(log_{10}\\) using\n\nlog10(df$AREA)\n\n [1] 2.712650 2.671228 2.664877 2.972481 3.132628 3.248870 3.226860 3.251952\n [9] 3.489968 3.599896 3.645898 3.648524 3.697481 3.648444 3.739631 3.873681\n[17] 3.853626 3.961418 4.005741 3.967908 4.137643 4.307513 4.392921 4.433674\n[25] 4.416937\n\n\nWe already the have the transformed data (\\(log_{10}\\)) ready on the variable LAREA and LESPECIES.\n\n\nModel with the transformed data\n\nmodel_transformed &lt;- lm(LSPECIES ~ LAREA, data = df)\n\n\n\nMain info about the new model\n\nsummary(model_transformed)\n\n\nCall:\nlm(formula = LSPECIES ~ LAREA, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.29609 -0.03734  0.01186  0.06469  0.13158 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept) -0.27342    0.13846  -1.975         0.0604 .  \nLAREA        0.38583    0.03777  10.215 0.000000000511 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09921 on 23 degrees of freedom\nMultiple R-squared:  0.8194,    Adjusted R-squared:  0.8115 \nF-statistic: 104.4 on 1 and 23 DF,  p-value: 0.0000000005107\n\n\n\n\nPlot of the regression line\n\nplot2 &lt;- ggplot(df, aes(x = LAREA, y = LSPECIES)) +\n  geom_point() +                        # Scatter plot\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")  \nplot2\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nResiduals vs. fitted data (transformed data)\n\nplot(model_transformed, which = 1)\n\n\n\n\n\n\nComparing the regression lines\n\ngridExtra::grid.arrange(plot1, plot2, ncol=2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nComparing the residuals\nLet’s check the difference in the residuals with non transformed and transformed data.\n\n# residuals versus fitted of non transformed data\nplot(model, which = 1)\n\n\n\n# residuals versus fitted of transformed data\nplot(model_transformed, which = 1)\n\n\n\n\n\n\nCook’s Distance\nWe will analyse now another measure called Cook’s Distance. It is used to help identify influential data points that have a significant impact on the fitted model. It combines information on both leverage (how far the independent variable is from the mean) and residuals (how far the predicted value is from the actual value).\nWhat Cook’s Distance represents:\n\nInfluence: It shows how much the estimated regression coefficients (the slope and intercept) would change if a particular data point were removed from the analysis.\nA large Cook’s Distance indicates that a point is influential, meaning it has a considerable impact on the regression model, and removing it could lead to a significantly different model.\n\n\n\n\n\n\n\nInterpretation:\n\n\n\n\nA general rule of thumb is that points with a Cook’s Distance greater than 1 may be considered influential.\nHowever, this threshold can vary depending on the dataset and the number of observations. For smaller datasets, a lower threshold might be more appropriate.\n\n\n\nWe can get the values using the following function\n\n# get the values and add in the dataset\ndf$cook &lt;- cooks.distance(model_transformed)\n\nLet’s see the first 5 variables and the cook’s distance values.\n\nDT::datatable(df[,c(1:5,15)])\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we observed\n\n\n\nThe first value LAREA = 2.71 and LSPECIES = 0.47 has a Cook’s Distance of 1.022, which is higher than 1, considered influential.\nHowever, deciding whether to delete a point based solely on Cook’s Distance requires careful consideration of the context and the nature of the data. Here’s when and how to evaluate whether to delete a point with a high Cook’s Distance value:\n\nCheck for Data Entry Errors: If the data point with high Cook’s Distance is a result of an entry mistake (e.g., a typo or incorrect measurement), you can justify removing or correcting it.\nCheck for Validity: Ensure the data point is valid and not an outlier due to faulty data collection or recording. If it’s valid, you might consider keeping it, but see if it warrants a different treatment (such as robust regression).\n\nBased on these considerations, we decided not to delete this point.\n\n\nWe can also plot these values.\n\nplot(model_transformed, which = 4)\n\n\n\n\n\n\nCreating a new column based on a unity transformation\nNow, we will change the air unit from \\(cm^2\\) to \\(dm^2\\) and insert in a new column called LDMAREA.\nWe will apply the following formula:\n\\(log_{10}\\) of (area / 10000)\n\ndf$LDMAREA &lt;- log10(df$AREA/10000)\n\n\n\nNew regression with transformed unit of area\n\nmodel_new_unit &lt;- lm(LSPECIES ~ LDMAREA, data = df)\n\nsummary(model_new_unit)\n\n\nCall:\nlm(formula = LSPECIES ~ LDMAREA, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.29609 -0.03734  0.01186  0.06469  0.13158 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.26991    0.02431   52.24 &lt; 0.0000000000000002 ***\nLDMAREA      0.38583    0.03777   10.21       0.000000000511 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09921 on 23 degrees of freedom\nMultiple R-squared:  0.8194,    Adjusted R-squared:  0.8115 \nF-statistic: 104.4 on 1 and 23 DF,  p-value: 0.0000000005107\n\n\n\n\n\n\n\n\nNote\n\n\n\nChecking the estimated coefficients, the new regression line can be expressed by\nLSPECIES = 1.26 + 0.38*LMDAREA\n\n\n\n\nAnalysis of variance of the model\n\nanova(model_new_unit)\n\nAnalysis of Variance Table\n\nResponse: LSPECIES\n          Df  Sum Sq Mean Sq F value          Pr(&gt;F)    \nLDMAREA    1 1.02705 1.02705  104.35 0.0000000005107 ***\nResiduals 23 0.22637 0.00984                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe variable LDMAREA has 1 degree of freedom (df) and the residuals/errors have 23 degrees of freedom."
  },
  {
    "objectID": "two-way-anova.html",
    "href": "two-way-anova.html",
    "title": "Two-way ANOVA (fixed effects)",
    "section": "",
    "text": "A two-way ANOVA (Analysis of Variance) is a statistical method used to examine the influence of two different categorical independent variables (factors) on a continuous dependent variable. It helps determine whether there are any statistically significant differences between the means of groups that are categorized based on these two factors. It also allows for the exploration of possible interactions between the two factors."
  },
  {
    "objectID": "two-way-anova.html#multiple-comparisons-tukey",
    "href": "two-way-anova.html#multiple-comparisons-tukey",
    "title": "Two-way ANOVA (fixed effects)",
    "section": "Multiple comparisons: Tukey",
    "text": "Multiple comparisons: Tukey\nThe Tukey multiple comparison test is used after conducting an ANOVA to identify which specific groups’ means are different from each other.\n\nTT &lt;- agricolae::HSD.test(model, trt = 'DENSITY')\nTT\n\n$statistics\n    MSerror Df    Mean       CV       MSD\n  0.1821655 16 1.47175 29.00006 0.7050067\n\n$parameters\n   test  name.t ntr StudentizedRange alpha\n  Tukey DENSITY   4         4.046093  0.05\n\n$means\n        EGGS       std r        se   Min   Max     Q25    Q50     Q75\n15 1.6775000 0.6710800 6 0.1742438 0.867 2.600 1.13300 1.7995 2.01600\n30 1.1883333 0.6265169 6 0.1742438 0.467 2.230 0.77500 1.1335 1.41625\n45 0.8961667 0.3765376 6 0.1742438 0.356 1.400 0.71100 0.8665 1.13825\n8  2.1250000 0.5303301 6 0.1742438 1.500 2.875 1.78125 2.0000 2.50000\n\n$comparison\nNULL\n\n$groups\n        EGGS groups\n8  2.1250000      a\n15 1.6775000     ab\n30 1.1883333     bc\n45 0.8961667      c\n\nattr(,\"class\")\n[1] \"group\"\n\n\nWe observe the assigned letters (a,b and c) for the categories of DENSITY.\nHow to interpret:\nCategories with the same letters present same mean. Categories 8 and 15 received the letter ‘a’, so it means that their means don’t differ. Category 8 and 45 don’t have any letter in common, which means that their mean is different. Categories 8 and 30 have different means too.\n\nVisualizations of the effect\n\nplot(effects::allEffects(model))"
  }
]